{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97fc897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression as sk_OLS\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f5f81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "val_data = pd.read_csv('data/val.csv')\n",
    "test_data = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "575532d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all train data (labelled and unlabelled)\n",
    "X_train    = train_data['Phrase']\n",
    "y_train    = train_data['Sentiment']\n",
    "# get only labelled train data\n",
    "mask = (y_train != -100)\n",
    "train_data_clean    = train_data[mask]\n",
    "X_train_clean    = X_train[mask]\n",
    "y_train_clean    = y_train[mask]\n",
    "\n",
    "# get val data\n",
    "X_val    = val_data['Phrase']\n",
    "y_val    = val_data['Sentiment']\n",
    "\n",
    "# get test data\n",
    "X_test     = test_data['Phrase']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ed1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', str(text), flags=re.MULTILINE)\n",
    "    texter = re.sub(r\"<br />\", \" \", text)\n",
    "    texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n",
    "    texter = re.sub('&#39;', \"\\\"\", texter)\n",
    "    texter = re.sub('\\n', \" \", texter)\n",
    "    texter = re.sub(' u ',\" you \", texter)\n",
    "    texter = re.sub('`',\"\", texter)\n",
    "    texter = re.sub(' +', ' ', texter)\n",
    "    texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n",
    "    texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n",
    "    texter = re.sub('&amp;', 'and', texter)\n",
    "    texter = re.sub('\\r', ' ',texter)\n",
    "    #added substitutions\n",
    "\n",
    "    #***********added substitutions***********\n",
    "    # remove all the special characters\n",
    "    texter = re.sub(r'\\W', ' ', texter)\n",
    "    # remove all single characters\n",
    "    texter = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', texter)\n",
    "    # Remove single characters from the start\n",
    "    texter = re.sub(r'\\^[a-zA-Z]\\s+', ' ', texter)\n",
    "    # Remove numbers\n",
    "    texter = re.sub(r'\\d+', ' ', texter)\n",
    "    # Converting to Lowercase\n",
    "    texter = texter.lower()\n",
    "    # Remove punctuation\n",
    "    texter = re.sub(r'[^\\w\\s]', ' ', texter)\n",
    "    # Remove parentheses\n",
    "    texter = re.sub(r'\\([^)]*\\)', ' ', texter)\n",
    "    # Remove single quotes\n",
    "    texter = re.sub(r'\\'', ' ', texter)\n",
    "    # Substituting multiple spaces with single space\n",
    "    texter = re.sub(r'\\s+', ' ', texter, flags=re.I)\n",
    "\n",
    "    clean = re.compile('<.*?>')\n",
    "    texter = texter.encode('ascii', 'ignore').decode('ascii')\n",
    "    texter = re.sub(clean, '', texter)\n",
    "    if texter == \"\":\n",
    "        texter = \"\"\n",
    "    return texter\n",
    "\n",
    "def clean_dataset(dataset):\n",
    "    for row in range(dataset.shape[0]):\n",
    "        dataset[row,0] = clean(dataset[row,0])\n",
    "    return dataset\n",
    "\n",
    "def tokenize_lexicon(texts):\n",
    "    return_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append(nltk.word_tokenize(texts[i]))\n",
    "        return_texts[i] = nltk.pos_tag(return_texts[i])\n",
    "    return return_texts\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN\n",
    "\n",
    "def lemmatize_texts(texts):\n",
    "    return_texts = []\n",
    "    lemmer = nltk.stem.WordNetLemmatizer()\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append([])\n",
    "        for j in range(len(texts[i])):\n",
    "                return_texts[i].append(lemmer.lemmatize(texts[i][j][0], pos=get_wordnet_pos(texts[i][j][1])))\n",
    "    return return_texts\n",
    "\n",
    "def stem_texts(texts):\n",
    "    return_texts = []\n",
    "    ps = PorterStemmer()\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append([])\n",
    "        for j in range(len(texts[i])):\n",
    "                return_texts[i].append(ps.stem(texts[i][j][0]))\n",
    "    return return_texts\n",
    "\n",
    "\n",
    "def backtostring(texts):\n",
    "    return_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append(\" \".join(texts[i]))\n",
    "    return return_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81f82cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data):\n",
    "    preproc_data = data.copy()\n",
    "    preproc_data = preproc_data.str.lower()\n",
    "    punctuation = string.punctuation\n",
    "    mapping = str.maketrans(\"\", \"\", punctuation)\n",
    "    preproc_data = preproc_data.str.translate(mapping)\n",
    "    #nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preproc_data = preproc_data.apply(lambda text: ' '.join([word for word in str(text).split() if word.lower() not in stop_words]))\n",
    "    #nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    preproc_data = preproc_data.apply(lambda text: ' '.join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "    preproc_data = preproc_data.apply(lambda text: re.sub(r'@\\w+', '', re.sub(r'http\\S+|www\\S+', '', text)))\n",
    "    return preproc_data\n",
    "\n",
    "# get the preprocessed data\n",
    "X_train_preproc   = pre_process(X_train)\n",
    "X_train_clean_preproc   = pre_process(X_train_clean)\n",
    "X_val_preproc = pre_process(X_val)\n",
    "X_test_preproc = pre_process(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd24a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([X_train_preproc, X_val_preproc, X_test_preproc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb31f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_word(data,  threshold_M):\n",
    "    vectorizer = CountVectorizer(binary=True, max_features= threshold_M)\n",
    "    vectorizer.fit(combined_data)\n",
    "    X = vectorizer.transform(data)\n",
    "    featurized_data = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "    return featurized_data\n",
    "\n",
    "# get the featurized data\n",
    "X_train   = bag_of_word(X_train_preproc, 125)\n",
    "X_train_clean = bag_of_word(X_train_clean_preproc, 125)\n",
    "X_val = bag_of_word(X_val_preproc, 125)\n",
    "X_test = bag_of_word(X_test_preproc, 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df812897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/k086shdn3v94mkr151z92xx40000gn/T/ipykernel_91509/2908842827.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train.loc[unlabeled_mask] = unlabeled_labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"pca = PCA(n_components=2)\\nX_train_2d = pca.fit_transform(X_train_bow)  # Apply PCA to the entire BoW data for visualization\\ncluster_centers_2d = pca.transform(kmeans.cluster_centers_)\\n\\nplt.figure(figsize=(10, 6))\\nplt.scatter(X_train_2d[:, 0], X_train_2d[:, 1], c=y_train, cmap='viridis', alpha=0.5, label='Data points')\\nplt.scatter(cluster_centers_2d[:, 0], cluster_centers_2d[:, 1], marker='D', c='red', s=100, label='Cluster centers')\\nplt.xlabel('Principal Component 1')\\nplt.ylabel('Principal Component 2')\\nplt.title('K-Means Clustering with PCA-reduced Data')\\nplt.legend()\\nplt.grid(True)\\nplt.show()\\n\\n# Elbow Method for selecting optimal k\\nobjective_scores = []\\nk_values = range(1, 21)\\nfor k in k_values:\\n    kmeans = KMeans(n_clusters=k, random_state=0)\\n    kmeans.fit(X_train_2d)\\n    objective_scores.append(kmeans.inertia_)\\n\\nplt.figure(figsize=(10, 6))\\nplt.plot(k_values, objective_scores, marker='o', linestyle='-')\\nplt.xlabel('Number of Clusters (k)')\\nplt.ylabel('K-Means Objective (Inertia)')\\nplt.title('Elbow Method for Optimal k')\\nplt.xticks(k_values)\\nplt.grid(True)\\nplt.show()\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BAG OF WORDS NAMES 125 threshold for pca\n",
    "X_train_bow = bag_of_word(X_train_preproc, 125)\n",
    "X_train_clean_bow = bag_of_word(X_train_clean_preproc, 125)\n",
    "X_val_bow = bag_of_word(X_val_preproc, 125)\n",
    "X_test_bow = bag_of_word(X_test_preproc, 125)\n",
    "\n",
    "# do kmeans only on unlabeled data denoted by -100, this one is not the optimal but had to explore kmeans on only unlabeled\n",
    "unlabeled_mask = (y_train == -100)\n",
    "X_train_unlabeled_bow = X_train_bow[unlabeled_mask]\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "kmeans.fit(X_train_unlabeled_bow)\n",
    "unlabeled_labels = kmeans.labels_\n",
    "#ASSIGNING pseudo-labels to the unlabeled data in y_train\n",
    "y_train.loc[unlabeled_mask] = unlabeled_labels\n",
    "\n",
    "# visualizatio\n",
    "\"\"\"pca = PCA(n_components=2)\n",
    "X_train_2d = pca.fit_transform(X_train_bow)  # Apply PCA to the entire BoW data for visualization\n",
    "cluster_centers_2d = pca.transform(kmeans.cluster_centers_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train_2d[:, 0], X_train_2d[:, 1], c=y_train, cmap='viridis', alpha=0.5, label='Data points')\n",
    "plt.scatter(cluster_centers_2d[:, 0], cluster_centers_2d[:, 1], marker='D', c='red', s=100, label='Cluster centers')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('K-Means Clustering with PCA-reduced Data')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Elbow Method for selecting optimal k\n",
    "objective_scores = []\n",
    "k_values = range(1, 21)\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(X_train_2d)\n",
    "    objective_scores.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, objective_scores, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('K-Means Objective (Inertia)')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.show()\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "601f69aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_labeled_class_counts = pd.Series(unlabeled_labels).value_counts()\n",
    "combined_y_train = pd.concat([y_train_clean, pd.Series(unlabeled_labels)], ignore_index=True)\n",
    "combined_X_train = pd.concat([X_train_clean_bow, X_train_unlabeled_bow], ignore_index=True)  # Assuming bow vectors for both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3262378e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'class_weight': ['balanced', 'balanced_subsample'],\n",
       "                         'max_depth': [None, 10, 20, 30],\n",
       "                         'n_estimators': [10, 50, 100]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#GRID SEARCH TEST\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=0),\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=5,  \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "rf_grid_search.fit(combined_X_train, combined_y_train)\n",
    "\n",
    "# best parameters end up being max depth 10, n_estimators 100, meaning to less depth, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb84efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/k086shdn3v94mkr151z92xx40000gn/T/ipykernel_91509/2269127403.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train.loc[unlabeled_mask] = all_labels[unlabeled_mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for RF ,pca bow ,+ kmeans: 0.39090987272101824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "X_train_bow = bag_of_word(X_train_preproc, 125)\n",
    "X_train_clean_bow = bag_of_word(X_train_clean_preproc, 125)\n",
    "X_val_bow = bag_of_word(X_val_preproc, 125)\n",
    "X_test_bow = bag_of_word(X_test_preproc, 125)\n",
    "\n",
    "pca = PCA(n_components=125, random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train_bow)\n",
    "X_val_pca = pca.transform(X_val_bow)\n",
    "X_test_pca = pca.transform(X_test_bow)\n",
    "#kmeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)  \n",
    "kmeans.fit(X_train_pca)\n",
    "all_labels = kmeans.labels_\n",
    "\n",
    "#assigning our pesudo labels for unlabeled data in y train using KMeans amd update y train for unlabeled\n",
    "y_train.loc[unlabeled_mask] = all_labels[unlabeled_mask]\n",
    "combined_X_train = X_train_pca  \n",
    "combined_y_train = y_train \n",
    "\n",
    "# RANDOM FOREST \n",
    "rf_classifier = RandomForestClassifier(class_weight=\"balanced\", random_state=0, n_estimators=10)\n",
    "rf_classifier.fit(combined_X_train, combined_y_train)\n",
    "\n",
    "# Accuracy is .39\n",
    "y_val_pred = rf_classifier.predict(X_val_pca)\n",
    "print(\"Accuracy for RF ,pca bow ,+ kmeans:\", accuracy_score(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*keepdims.*\")\n",
    "\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier.fit(combined_X_train, combined_y_train)\n",
    "\n",
    "\n",
    "y_val_pred = knn_classifier.predict(X_val_pca)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Accuracy on Validation Set with k=3, pca, bow, kmeans, rf: {accuracy}\")\n",
    "#.42 is best with these methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f1f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "#optinized using grid search \n",
    "optimized_rf_classifier = RandomForestClassifier(class_weight=\"balanced\", random_state=0, \n",
    "                                                 n_estimators=100, max_depth=10)\n",
    "optimized_rf_classifier.fit(combined_X_train, combined_y_train)\n",
    "\n",
    "# Random Forest on validation set\n",
    "y_val_pred_rf = optimized_rf_classifier.predict(X_val_pca)\n",
    "print(\" RF Accuracy on Val class 1 and 4 always worst:\", accuracy_score(y_val, y_val_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94741077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
